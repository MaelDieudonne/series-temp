---
title: "Projet de séries temporelles"
author: "Maël Dieudonné"
date: "2024-05-13"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
---

# Pré-requis

```{r, setup}
packages <- c("aTSA", "forecast", "ggplot2", "gridExtra", "xts")
installed_packages <- packages %in% rownames(installed.packages())

if (any(installed_packages == FALSE)) {install.packages(packages[!installed_packages])}
invisible(lapply(packages, library, character.only = TRUE))

rm(installed_packages, packages)
```

## Téléchargement puis extraction des données

La série étudiée est l'indice CVS-CJO de la production industrielle dans l'industrie pharmaceutique (base 100 en 2021).

```{r}
file_url <- "https://www.insee.fr/fr/statistiques/serie/telecharger/csv/010767832?ordre=antechronologique&transposition=donneescolonne&periodeDebut=1&anneeDebut=1990&periodeFin=2&anneeFin=2024&revision=sansrevisions"
name <- "Production de l'industrie pharmaceutique"

local_file_name <- "data.zip"
download.file(file_url, local_file_name, mode = "wb", quiet = TRUE)

file_list <- unzip(local_file_name)
file.rename(file_list[2], "valeurs_mensuelles.csv")

file.remove("data.zip")
file_list[2] <- sub("^\\.\\/", "", file_list[2])
file_list[2] <- sub("/valeurs_mensuelles\\.csv$", "", file_list[2]) 
unlink(file_list[2], recursive = TRUE)

rm(file_url, local_file_name, file_list)
```

## Préparation des données

On supprime l'entête, on s'assure du nom et du format des colonnes, on tronque la série en décembre 2019 (pour éliminer les fluctuation intempestives dues au covid) et on la convertit au format ts.

```{r}
data <- read.csv("valeurs_mensuelles.csv", sep = ";")
data <- data[-c(1:3), -3]
colnames(data) <- c("dates", "values")

data <- data[order(data$dates), ]
rownames(data) <- NULL

data$dates <- as.yearmon(data$dates)
data$values <- as.numeric(data$values)

data <- data[1:which(data$dates == "Dec 2019"), ]

raw_series <- ts(data$values, start = min(data$dates), frequency = 12)
```

# Partie I : données

## 1. Représentation de la série

```{r}
autoplot(raw_series, xlab = "", ylab = "") +
  labs(title = name, subtitle = "Série brute") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))

autoplot(decompose(raw_series), xlab = "") +
  labs(title = name, subtitle = "Série brute – Décomposition") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))
```

On remarque une tendance linéaire croissante, une augmentation progressive de la variance, et l'absence de saisonnalité.

## 2. Stationnarisation

On commence par une transformation logarithmique pour corriger l'hétéroscédasticité.

```{r}
log_series <- log(raw_series)

autoplot(decompose(log_series), xlab = "") +
  labs(title = name, subtitle = "Série log-transformée – décomposition") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))
```

On différencie ensuite à l'ordre 1 pour éliminer la tendance.

```{r}
diff_series <- diff(log_series, 1)

autoplot(decompose(diff_series), xlab = "") +
  labs(title = name, subtitle = "Série log-transformée puis 1-différenciée – décomposition") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))
```

On vérifie que le résultat convient à l'aide des tests usuels de stationnarité. La tendance a disparu mais il reste une légère dérive. On retient donc les tests avec dérive et sans tendance.

```{r}
cat(paste("Dérive de la série :", mean(diff_series), "\n"))

adf <- adf.test(coredata(diff_series), nlag = 24, output = FALSE)
pp <- pp.test(coredata(diff_series), lag.short = TRUE, output = FALSE)
kpss <- kpss.test(coredata(diff_series), lag.short = TRUE, output = FALSE)

cat("\nTest ADF with drift no trend\n")
print(adf$type2)
cat("\nTest PP with drift no trend\n")
print(pp["type 2", ])
cat("\nTest KPSS with drift no trend\n")
print(kpss["type 2", ])
```

Les résultats permettent de rejeter l'hypothèse nulle de non-stationnarité pour les test ADF et PP, mais pas l'hypothèse nulle de stationnarité avec le test KPSS.

## 3. Comparaison de la série initiale et de la série transformée

```{r}
plot1 <- autoplot(raw_series, xlab = "", ylab = "") +
  labs(title = "Série brute") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

plot2 <- autoplot(diff_series, xlab = "", ylab = "") +
  labs(title = "Série transformée") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

grid.arrange(plot1, plot2, ncol = 1)
```

# Partie II : Modèle ARMA

## 1. Identification des paramètres

On représente les fonctions d'auto-corrélation et d'auto-corrélation partielle.

```{r}
plot1 <- ggAcf(diff_series) +
  labs(title = "Autocorrélations", x = NULL, y = NULL) +
  theme_minimal() +
  theme(plot.title = ggplot2::element_text(hjust = 0.5))

plot2 <- ggPacf(diff_series) +
  labs(title = "Autocorrélations partielles", x = NULL, Y = NULL) +
  theme_minimal() +
  theme(plot.title = ggplot2::element_text(hjust = 0.5))

grid.arrange(plot1, plot2, ncol = 1)
```

On retient les valeurs de 1 pour $q$ et 11 pour $p$. Ce n'est pas l'approche la plus parcimonieuse, mais le pic apparaissant au 11e retard pour les auto-corrélations partielles est bien prononcé (s'il apparaissait au 12e, on se serait plutôt inquiété d'une saisonnalité non-corrigée).

## 2. Estimation des modèles

On compare les modèles possibles selon les critères d'information : c'est un $AR(11)$ qui minimise l'AIC et un $MA(1)$ qui minimise le BIC (ce qui est logique puisque le BIC pénalise davantage la complexité).

```{r}
pmax <- 11
qmax <- 1
mat <- matrix(NA, nrow=pmax+1,ncol=qmax+1)
rownames(mat) <- paste0("p=",0:pmax)
colnames(mat) <- paste0("q=",0:qmax)
AICs <- mat
BICs <- mat

pqs <- expand.grid(0:pmax,0:qmax)
for (row in 1:dim(pqs)[1]){
  p <- pqs[row,1]
  q <- pqs[row,2]
  estim <- try(arima(log_series,c(p,1,q),include.mean = F))
  AICs[p+1,q+1] <- if (class(estim)=="try-error") NA else estim$aic
  BICs[p+1,q+1] <- if (class(estim)=="try-error") NA else BIC(estim)
}

print(AICs)
lowest_cell <- which(as.matrix(AICs) == min(as.matrix(AICs)), arr.ind = TRUE)
p <- rownames(AICs)[lowest_cell[1]]
p <- sub("^p=", "", p)
q <- colnames(AICs)[lowest_cell[2]]
q <- sub("^q=", "", q)
cat(paste0("\nBest model according to AIC is an ARMA(",p,",",q,").\n\n"))

print(BICs)
lowest_cell <- which(as.matrix(BICs) == min(as.matrix(BICs)), arr.ind = TRUE)
p <- rownames(BICs)[lowest_cell[1]]
p <- sub("^p=", "", p)
q <- colnames(BICs)[lowest_cell[2]]
q <- sub("^q=", "", q)
cat(paste0("\nBest model according to BIC is an ARMA(",p,",",q,")."))
```

On compare les modèles possibles selon l'erreur de prédiction calculées sur 5 dernières valeurs : c'est l'$ARMA(11,0)$ qui la minimise.

```{r}
values <- data$values[(nrow(data) - 4):nrow(data)]
pqs <- expand.grid(0:pmax,0:qmax)
mat <- matrix(NA, nrow=pmax+1, ncol=qmax+1)
rownames(mat) <- paste0("p=",0:pmax)
colnames(mat) <- paste0("q=",0:qmax)
pred_error <- mat 
for (row in 1:dim(pqs)[1]){
  p <- pqs[row,1]
  q <- pqs[row,2]
  pred_error[p+1,q+1] <- sqrt(mean((predict(arima(log_series,c(p,1,q),include.mean=F),n.ahead = 5)$pred[1:5] - values)^2))
}

pred_error
lowest_cell <- which(as.matrix(pred_error) == min(as.matrix(pred_error)), arr.ind = TRUE)
p <- rownames(BICs)[lowest_cell[1]]
p <- sub("^p=", "", p)
q <- colnames(BICs)[lowest_cell[2]]
q <- sub("^q=", "", q)
cat(paste0("\nBest model according to prediction error is an ARMA(",p, ",", q, ")."))
```

On retient ces deux modèles afin de vérifier leur validité et leur ajustement.

```{r}
arima1110 <- arima(log_series, c(11,1,0), include.mean=F)
arima011 <- arima(log_series, c(0,1,1), include.mean=F)
```

## 3. Tests de validité et d'ajustement

Concernant d'abord l'ajustement, on vérifie la significativité des coefficients les plus élevés : les modèles $AR(11)$ et $MA(1)$ passent ce test.

```{r}
signif <- function(estim) {
  coef <- estim$coef
  se <- sqrt(diag(estim$var.coef))
  t <- coef/se
  pval <- (1 - pnorm(abs(t)))*2
  return(rbind(coef, se, pval))
}

signif(arima1110)
cat("\n")
signif(arima011)
```

On considère maintenant la validité, c'est-à-dire l'absence d'autocorrélation entre les résidus. On utilise un test Portemanteau, conduisant à rejeter le $MA(1)$.

```{r}
Qtests <- function(series, k, fitdf = 0) {
  pvals <- apply(matrix(1:k), 1, FUN = function(l) {
    pval <- if (l <= fitdf) NA else Box.test(series, lag = l, type = "Ljung-Box", fitdf = fitdf)$p.value
    return(c("lag" = l,"pval" = pval))
  })
  return(t(pvals))
}

cat("ARMA(11,0)\n")
Qtests(arima1110$residuals, 24, fitdf=11)
cat("\nARMA(0,1)\n")
Qtests(arima011$residuals, 24, fitdf=1)
```

On procède à une dernière vérification avec la fonction auto.arima() du package forecast : elle retient le $MA(1)$ que l'on vient d'écarter.

```{r}
auto.arima(log_series, trace = TRUE)
```

# Partie III : Prévision

## 1. Région de confiance de niveau $\alpha$

Le modèle $AR(11)$ retenu s'écrit ainsi :

$$
\nabla X_{t+1} = ar_1 \nabla X_t + ar_2 \nabla X_{t-1} + \cdots + ar_{11} \nabla X_{t-10} + \varepsilon_t
$$

On en déduit l'expression des termes prédits :

$$
\hat{X}_{t+1} = \mathbb{E}[X_{t+1}|X_t,\ldots,X_{t-11}] = X_t + \hat{ar}_1(X_t - X_{t-1}) + \hat{ar}_2(X_{t-1} - X_{t-2}) + \cdots + \hat{ar}_{11}(X_{t-10} - X_{t-11}) \\
\hat{X}_{t+2} = \mathbb{E}[X_{t+2}|X_t,\ldots,X_{t-10}] = \hat{X}_{t+1} + \hat{ar}_1(\hat{X}_{t+1} - X_{t}) + \hat{ar}_2(X_{t} - X_{t-1}) + \cdots + \hat{ar}_{11}(X_{t-9} - X_{t-10}) \\
\cdots
$$

Pour calculer la variance, on pose $ar_0 = 1$ et on élimine les termes connus plutôt que prédits lorsque l'horizon $h$ est inférieur à 11.

$$
\hat{\mathbb{V}}[\hat{X}_{t+h}] = (\hat{ar}_{0}^2+\hat{ar}_{1}^2)\hat{\mathbb{V}}[\hat{X}_{t+h-1}] + (\hat{ar}_{1}^2+\hat{ar}_{2}^2)\hat{\mathbb{V}}[\hat{X}_{t+h-2}] + \cdots + (\hat{ar}_{10}^2+\hat{ar}_{11}^2)\hat{\mathbb{V}}[\hat{X}_{t+h-11}] + \mathbb{V}[\varepsilon_t]
$$

On obtient la région de confiance de niveau $\alpha$ :

$$
IC_{0.95}(\mathbb{V}[\hat{X}_{t+h}]) = \left[ \hat{X}_{t+h} \pm 1.96\sqrt{\hat{\mathbb{V}}[\hat{X}_{t+h}]} \right]
$$

## 2. Hypothèses

Les calculs précédents requièrent plusieurs hypothèses.

-   Le modèle est parfaitement connu.

-   On considère les coefficients estimés comme les vrais coefficients du modèle. Ceci permet de négliger l'incertitude qui les entoure, pour considérer seulement la variance des résidus et des prévisions précédentes lors du calcul de la variance. C'est une pratique habituelle, car la variance des coefficients estimés est généralement très inférieure à celle des résidus. Cela n'est pas le cas ici, mais revenir sur cette hypothèse complexifierait de manière déraisonnable les calculs.

```{r}
cat("Variance des résidus =", arima1110$sigma2, "\n")
cat("Variance des coefficients = \n")
print(diag(arima1110$var.coef))
```

## 3. Représentation graphique

On commence par préparer un dataframe pour stocker les valeurs prédites et les intervalles de confiance.

```{r}
data$log_values <- log(data$values)
data$var <- NA
data$CIlow <- NA
data$CIup <- NA
data$CIlow[nrow(data)] <- data$values[nrow(data)]
data$CIup[nrow(data)] <- data$values[nrow(data)]

arima1110$coef["ar0"] <- 1
```

On écrit une fonction qui élimine automatiquement les coefficients non-significatifs au seuil de 5 %.

```{r}
forecast <- function(h) {
  # On isole les coefficients statistiquement significatifs.
  sig_coefs <- c()
  for (i in 1:11) {
    coef <- arima1110$coef[paste0("ar", i)]
    se <- sqrt(arima1110$var.coef[paste0("ar", i), paste0("ar", i)])
    t <- coef/se
    pval <- (1 - pnorm(abs(t)))*2
    if (pval <= 0.05) {sig_coefs <- c(sig_coefs, i)}
    else {cat("ar", i, " rejected, p-value = ", pval, "\n", sep="")}
  }
  # On itère sur les horizons de prévision.
  for (j in 1:h) {
    T <- nrow(data)
    # On itère sur les coefficients significatifs pour calculer la valeur prédite.
    prev <- data$log_values[T]
    for (k in sig_coefs) {prev <- prev + as.numeric(arima1110$coef[paste0("ar", k)]) * (data$log_values[T-k+1] - data$log_values[T-k])}
    # On calcule la variance puis l'intervalle de confiance.
    if (j == 1) {var <- arima1110$sigma2}
    if (j > 1) {
      var <- 0
      for (l in 0:(j-2)) {var <- var + ((arima1110$coef[paste0("ar", l)]**2 + arima1110$coef[paste0("ar", l+1)]**2) * data$var[T-l]) + arima1110$sigma2}
    }
    CIlow <- prev - 1.96*sqrt(var)
    CIup <- prev + 1.96*sqrt(var)
    data <<- rbind(data, data.frame(
      dates = (max(data$dates)) + (1/12), 
      values = exp(prev), 
      log_values = prev, 
      var = var, 
      CIlow = exp(CIlow), 
      CIup = exp(CIup)))
  }
  cat("\nForecast complete for h = ", h, ".", sep="")
}

forecast(6)
rownames(data) <- NULL
print(tail(data[, -which(names(data) == "log_values")], 10))
```

On représente finalement les résultats. Le paramètre $r$ permet de choisir la durée représentée, prévisions incluses, soit ici 24 mois d'observations et 6 mois de prévision. On applique une transformation exponentielle aux prévisions et aux bornes de l'intervalle de confiance pour retrouver des valeurs cohérentes avec la série initiale.

```{r, warning=FALSE}
r <- 30
subset_data <- data[(nrow(data) - r):nrow(data), ]

ggplot(data = subset_data) +
  geom_line(data = subset_data[0:(nrow(subset_data)-6), ], aes(x=dates, y=values, color = "obs")) +
  geom_line(data = subset_data[(nrow(subset_data)-6):nrow(subset_data), ], aes(x=dates, y=values, color = "prev")) +
  geom_line(aes(x=dates, y = CIlow), color = "limegreen", linetype = "dashed") +
  geom_line(aes(x=dates, y = CIup), color = "limegreen", linetype = "dashed") +
  geom_ribbon(aes(x=dates, ymin = CIlow, ymax = CIup, fill = "IC"), alpha = 0.5) +
  scale_color_manual(values = c("obs" = "skyblue", "prev" = "darkorange"), labels = c("obs" = "Observations", "prev" = "Prévisions")) +
  scale_fill_manual(values = c("IC" = "lightgray"), labels = c("IC" = "IC 95 %")) +
  labs(
    title = "Production de l'industrie pharmaceutique", 
    subtitle = "Prévisions à 6 mois", 
    x = "",
    y = "", 
    color = "",
    fill = "") +
  guides(color = guide_legend(order = 1), fill = guide_legend(order = 2)) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5), 
    plot.subtitle = element_text(hjust = 0.5), 
    legend.position = "bottom")
```

## 4. Question ouverte

Imaginons qu'il existe une série $Y_t$ stationnaire entre $t=1$ et $T$ et telle que $Y_{T+1}$ soit disponible plus rapidement que $X_{T+1}$ : $Y_{T+1}$ permet d'améliorer la prévision de $X_{T+1}$ si $(Y_t)$ est corrélée instantanément avec $(X_t)$. Cette relation s'écrit :

$$
\hat{Y}_{T+1|\{X_u,Y_u,u≤t\}∪\{X_{T+1}\}}≠\hat{Y}_{T+1|\{Y_u,u≤t\}}
$$

On peut vérifier cette hypothèse à l'aide d'un test de Wald.
