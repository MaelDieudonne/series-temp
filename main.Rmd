---
title: "Projet de séries temporelles"
author: "Maël Dieudonné"
date: "2024-05-13"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    keep_tex: true
---

# Pré-requis

```{r, setup, message=FALSE}
packages <- c("aTSA", "forecast", "ggplot2", "gridExtra", "xts")
installed_packages <- packages %in% rownames(installed.packages())

if (any(installed_packages == FALSE)) {install.packages(packages[!installed_packages])}
invisible(lapply(packages, library, character.only = TRUE))

rm(installed_packages, packages)
```

## Téléchargement puis extraction des données

La série étudiée est l'indice CVS-CJO de la production industrielle dans l'industrie pharmaceutique (base 100 en 2021).

```{r eval=FALSE}
file_url <- "https://www.insee.fr/fr/statistiques/serie/telecharger/csv/010767832?ordre=antechronologique&transposition=donneescolonne&periodeDebut=1&anneeDebut=1990&periodeFin=2&anneeFin=2024&revision=sansrevisions"
local_file_name <- "data.zip"
download.file(file_url, local_file_name, mode = "wb", quiet = TRUE)

file_list <- unzip(local_file_name)
file.rename(file_list[2], "valeurs_mensuelles.csv")

file.remove("data.zip")
file_list[2] <- sub("^\\.\\/", "", file_list[2])
file_list[2] <- sub("/valeurs_mensuelles\\.csv$", "", file_list[2]) 
unlink(file_list[2], recursive = TRUE)

rm(file_url, local_file_name, file_list)
```

## Préparation des données

On supprime l'entête, on s'assure du nom et du format des colonnes, on tronque la série en décembre 2019 (pour éliminer les fluctuation intempestives dues au covid) et on la convertit au format ts.

```{r}
data <- read.csv("valeurs_mensuelles.csv", sep = ";")
data <- data[-c(1:3), -3]
colnames(data) <- c("dates", "values")

data <- data[order(data$dates), ]
rownames(data) <- NULL

data$dates <- as.yearmon(data$dates)
data$values <- as.numeric(data$values)

data <- data[1:which(data$dates == "Dec 2019"), ]

raw_series <- ts(data$values, start = min(data$dates), frequency = 12)

name <- "Production de l'industrie pharmaceutique"
```

# Partie I : données

## 1. Représentation de la série

```{r}
autoplot(raw_series, xlab = "", ylab = "") +
  labs(title = name, subtitle = "Série brute") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))

autoplot(decompose(raw_series), xlab = "") +
  labs(title = name, subtitle = "Série brute – Décomposition") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))
```

On remarque une tendance linéaire croissante, une augmentation progressive de la variance, et l'absence de saisonnalité.

## 2. Stationnarisation

On commence par une transformation logarithmique pour corriger l'hétéroscédasticité.

```{r}
log_series <- log(raw_series)

autoplot(decompose(log_series), xlab = "") +
  labs(title = name, subtitle = "Série log-transformée – décomposition") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))
```

On différencie ensuite à l'ordre 1 pour éliminer constante et tendance.

```{r}
diff_series <- diff(log_series, 1)

autoplot(decompose(diff_series), xlab = "") +
  labs(title = name, subtitle = "Série log-transformée puis 1-différenciée – décomposition") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))
```

On vérifie que la constante et la tendance ont bien disparu à l'aide d'une régression linéaire : on peut effectivement rejeter les trois hypothèses de nullité de l'intercept (p-value = 0,279), du coefficient du temps (p-value = 0,677), et de nullité jointe (p-value = 0,6767).

```{r}
time <- 1:length(coredata(diff_series))
regression <- lm(coredata(diff_series) ~ time)
summary(regression)
```

On procède aux tests usuels de stationnarité sans dérive ni tendance (= type 1 dans les fonctions du package aTSA qu'on utilise ici).

```{r}
adf <- adf.test(coredata(diff_series), nlag = 24, output = FALSE)
pp <- pp.test(coredata(diff_series), lag.short = TRUE, output = FALSE)
kpss <- kpss.test(coredata(diff_series), lag.short = TRUE, output = FALSE)

cat("\nTest ADF with no drift nor trend\n")
print(adf$type1)
cat("\nTest PP with no drift nor trend\n")
print(pp["type 1", ])
cat("\nTest KPSS with no drift nor trend\n")
print(kpss["type 1", ])
```

Les résultats permettent de rejeter l'hypothèse nulle de non-stationnarité des tests ADF et PP au seuil de 1 %. En revanche, ils conduisent à rejeter l'hypothèse nulle de stationnarité du test KPSS au seuil de 1 %.

Une explication possible à cette divergence réside dans la construction des tests : les tests ADF et PP considèrent seulement la présence d'une racine unitaire, tandis que le test KPSS réagit aux évolutions de la variance. Il se pourrait donc que la transformation logarithmique n'ait pas suffit à éliminer l'hétéroscédasticité de la série initiale.

Une autre explication serait la persistance d'une légère dérive ou tendance malgré la différenciation, de sorte que les tests seraient mal spécifiés. On remarque en effet que les tests KPSS avec dérive et/ou tendance (= type 2 ou 3) permettent de rejeter l'hypothèse nulle de stationnarité, tandis que les tests ADF et PP demeurent concluants.

On admet alors, devant ces résultats, que la série transformée est stationnaire (d'autant que les résidus semblent bien distribués normalement).

```{r}
checkresiduals(diff_series, plot = TRUE)
```

## 3. Comparaison de la série initiale et de la série transformée

```{r}
plot1 <- autoplot(raw_series, xlab = "", ylab = "") +
  labs(title = "Série brute") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

plot2 <- autoplot(diff_series, xlab = "", ylab = "") +
  labs(title = "Série transformée") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

grid.arrange(plot1, plot2, ncol = 1)
```

# Partie II : Modèle ARMA

## 1. Identification des paramètres

On représente les fonctions d'auto-corrélation et d'auto-corrélation partielle.

```{r}
plot1 <- ggAcf(diff_series) +
  labs(title = "Autocorrélations", x = NULL, y = NULL) +
  theme_minimal() +
  theme(plot.title = ggplot2::element_text(hjust = 0.5))

plot2 <- ggPacf(diff_series) +
  labs(title = "Autocorrélations partielles", x = NULL, Y = NULL) +
  theme_minimal() +
  theme(plot.title = ggplot2::element_text(hjust = 0.5))

grid.arrange(plot1, plot2, ncol = 1)
```

On retient les valeurs de 1 pour $q$ et 11 pour $p$.

-   Concernant les autocorrélations, on néglige le pic apparaissant au 12 retard, car on sait qu'elles décroissent de manière exponentielle pour les processus ARMA, ce qui rend l'hypothèse $q=12$ extrêmement improbable.

-   Concernant les autocorrélations partielles, le pic apparaissant au 11e retard est bien prononcé : il semble préférable d'en tenir compte, même si cela n'est pas l'approche la plus parcimonieuse (s'il apparaissait au 12e retard, on se serait plutôt inquiété d'une saisonnalité non-corrigée).

## 2. Estimation des modèles

On compare les modèles possibles selon les critères d'information : c'est un $AR(11)$ qui minimise l'AIC et un $MA(1)$ qui minimise le BIC (ce qui est logique puisque le BIC pénalise davantage la complexité).

```{r}
pmax <- 11
qmax <- 1
mat <- matrix(NA, nrow=pmax+1,ncol=qmax+1)
rownames(mat) <- paste0("p=",0:pmax)
colnames(mat) <- paste0("q=",0:qmax)
AICs <- mat
BICs <- mat

pqs <- expand.grid(0:pmax,0:qmax)
for (row in 1:dim(pqs)[1]){
  p <- pqs[row,1]
  q <- pqs[row,2]
  estim <- try(arima(log_series,c(p,1,q),include.mean = F))
  AICs[p+1,q+1] <- if (class(estim)=="try-error") NA else estim$aic
  BICs[p+1,q+1] <- if (class(estim)=="try-error") NA else BIC(estim)
}

print(AICs)
lowest_cell <- which(as.matrix(AICs) == min(as.matrix(AICs)), arr.ind = TRUE)
p <- rownames(AICs)[lowest_cell[1]]
p <- sub("^p=", "", p)
q <- colnames(AICs)[lowest_cell[2]]
q <- sub("^q=", "", q)
cat(paste0("\nBest model according to AIC is an ARMA(",p,",",q,").\n\n"))

print(BICs)
lowest_cell <- which(as.matrix(BICs) == min(as.matrix(BICs)), arr.ind = TRUE)
p <- rownames(BICs)[lowest_cell[1]]
p <- sub("^p=", "", p)
q <- colnames(BICs)[lowest_cell[2]]
q <- sub("^q=", "", q)
cat(paste0("\nBest model according to BIC is an ARMA(",p,",",q,")."))
```

On compare les modèles possibles selon l'erreur de prédiction calculées sur les 5 dernières valeurs : c'est l'$ARMA(11,0)$ qui la minimise.

```{r}
values <- data$values[(nrow(data) - 4):nrow(data)]
pqs <- expand.grid(0:pmax,0:qmax)
mat <- matrix(NA, nrow=pmax+1, ncol=qmax+1)
rownames(mat) <- paste0("p=",0:pmax)
colnames(mat) <- paste0("q=",0:qmax)
pred_error <- mat 
for (row in 1:dim(pqs)[1]){
  p <- pqs[row,1]
  q <- pqs[row,2]
  pred_error[p+1,q+1] <- sqrt(mean((predict(arima(log_series,c(p,1,q),include.mean=F),n.ahead = 5)$pred[1:5] - values)^2))
}

pred_error
lowest_cell <- which(as.matrix(pred_error) == min(as.matrix(pred_error)), arr.ind = TRUE)
p <- rownames(BICs)[lowest_cell[1]]
p <- sub("^p=", "", p)
q <- colnames(BICs)[lowest_cell[2]]
q <- sub("^q=", "", q)
cat(paste0("\nBest model according to prediction error is an ARMA(",p, ",", q, ")."))
```

On retient ces deux modèles afin de vérifier leur validité et leur ajustement.

```{r}
arima1110 <- arima(log_series, c(11,1,0), include.mean=F)
arima011 <- arima(log_series, c(0,1,1), include.mean=F)
```

## 3. Tests de validité et d'ajustement

Concernant d'abord l'ajustement, on vérifie la significativité des coefficients les plus élevés : les modèles $AR(11)$ et $MA(1)$ passent ce test, avec des p-values de $<2.10^{-06}$ pour le coefficient $ar11$ et $10^{-08}$ pour le coefficient $ma1$.

```{r}
signif <- function(estim) {
  coef <- estim$coef
  se <- sqrt(diag(estim$var.coef))
  t <- coef/se
  pval <- (1 - pnorm(abs(t)))*2
  return(rbind(coef, se, pval))
}

signif(arima1110)
cat("\n")
signif(arima011)
```

On considère maintenant la validité, c'est-à-dire l'absence d'autocorrélation entre les résidus, avec un test Portemanteau dont elle constitue l'hypothèse nulle. Il valide le modèle $AR(11)$ en permettant de rejeter $H_0$ au seuil de 5 % pour l'ensemble des lags jusqu'à 24, mais récuse le modèle $MA(1)$ en ne permettant pas de rejeter $H_0$ au seuil de 5 % pour $h\in\{12,18\}$ et de 10 % pour $h\in\{4,5,20,21\}$.

```{r}
Qtests <- function(series, k, fitdf = 0) {
  pvals <- apply(matrix(1:k), 1, FUN = function(l) {
    pval <- if (l <= fitdf) NA else Box.test(series, lag = l, type = "Ljung-Box", fitdf = fitdf)$p.value
    return(c("lag" = l,"pval" = pval))
  })
  return(t(pvals))
}

cat("ARMA(11,0)\n")
Qtests(arima1110$residuals, 24, fitdf=11)
cat("\nARMA(0,1)\n")
Qtests(arima011$residuals, 24, fitdf=1)
```

On procède à une dernière vérification avec la fonction auto.arima() du package forecast : elle retient le $MA(1)$ que l'on vient d'écarter.

```{r}
auto.arima(log_series, trace = TRUE)
```

# Partie III : Prévision

## 1. Hypothèses

Les calculs précédents requièrent plusieurs hypothèses.

-   La structure du modèle est parfaitement connue.

-   Les coefficients estimés sont les vrais coefficients du modèle. Cette hypothèse permet de négliger l'incertitude qui les entoure, pour considérer seulement la variance des résidus et des prévisions précédentes lors du calcul de l'écart-type. C'est une pratique habituelle, car la variance des coefficients estimés est généralement très inférieure à celle des résidus. Cela n'est pas le cas ici, mais revenir sur cette hypothèse complexifierait de manière déraisonnable les calculs...

-   Les résidus sont gaussiens.

```{r}
# On constate ici que la variance des résidus et des coefficients estimés sont du même ordre de grandeur.
cat("Variance des résidus =", arima1110$sigma2, "\n")
cat("Variance des coefficients estimés = \n")
print(diag(arima1110$var.coef))
```

## 2. Région de confiance de niveau $\alpha$

Le modèle $AR(11)$ retenu s'écrit ainsi :

$$
\nabla X_{t+1} = ar_1 \nabla X_t + ar_2 \nabla X_{t-1} + \cdots + ar_{11} \nabla X_{t-10} + \varepsilon_t
$$

On en déduit l'expression des termes prédits à l'horizon 2 :

$$\hat{X}_{t+1} = \mathbb{E}[X_{t+1}|X_t,\ldots,X_{t-11}] = X_t + \hat{ar}_1(X_t - X_{t-1}) + \hat{ar}_2(X_{t-1} - X_{t-2}) + \cdots + \hat{ar}_{11}(X_{t-10} - X_{t-11})$$ $$\hat{X}_{t+2} = \mathbb{E}[X_{t+2}|X_t,\ldots,X_{t-10}] = \hat{X}_{t+1} + \hat{ar}_1(\hat{X}_{t+1} - X_{t}) + \hat{ar}_2(X_{t} - X_{t-1}) + \cdots + \hat{ar}_{11}(X_{t-9} - X_{t-10})$$

On calcule ensuite la variance en éliminant les termes certains, et en utilisant le fait que le bruit résiduel d'un processus $ARMA$ est décorrelé mais identiquement distribué entre les périodes :

$$
\hat{\mathbb{V}}[\hat{X}_{t+1}] = \mathbb{V}[\hat{X}_{t+1} + \varepsilon_{t+1}] = \mathbb{V}[\varepsilon_{t+1}]
$$

$$
\hat{\mathbb{V}}[\hat{X}_{t+2}] = \mathbb{V}[\hat{X}_{t+2} + \varepsilon_{t+2}] = \mathbb{V}[(1+\hat{ar}_{1})\varepsilon_{t+1} + \varepsilon_{t+2}] = (1+\hat{ar}_{1})^2*\mathbb{V}[\varepsilon_{t+1}] + \mathbb{V}[\varepsilon_{t+2}] = \left( (1+\hat{ar}_{1})^2 + 1 \right)*\mathbb{V}[\varepsilon_{t+1}]
$$

On en déduit finalement l'expression des intervalles de confiance :

$$
IC_{0.95}(\mathbb{V}[\hat{X}_{t+1}]) = \left[ \hat{X}_{t+1} \pm 1.96*\sigma \right]
$$

$$
IC_{0.95}(\mathbb{V}[\hat{X}_{t+2}]) = \left[ \hat{X}_{t+2} \pm \frac{1.96*\sigma}{\sqrt{(1+\hat{ar}_{1})^2 + 1}} \right]
$$

Dans le cas d'un $AR(11)$, ces formules se complexifient dramatiquement lorsqu'on allonge l'horizon de prédiction. On devrait obtenir le produit d'un polynôme $P$ des coefficients du modèle avec un bruit, permettant de se ramener à une loi normale centrer réduite pour déterminer les bornes de l'intervalle de confiance. Nous avons simplifié en calculant la variance de chaque prédiction de manière itérative, en reprenant la variance des prédictions précédentes, selon la formule suivante (où $ar_0 = 1$ et où les termes certains disparaissent lorsque l'horizon $h$ est inférieur à 11) :

$$
\hat{\mathbb{V}}[\hat{X}_{t+h}] = (ar_0+\hat{ar}_{1})^2*\hat{\mathbb{V}}[\hat{X}_{t+h-1}] + (\hat{ar}_{1}+\hat{ar}_{2})^2*\hat{\mathbb{V}}[\hat{X}_{t+h-2}] + \cdots + (\hat{ar}_{10}+\hat{ar}_{11})^2*\hat{\mathbb{V}}[\hat{X}_{t+h-11}] + \mathbb{V}[\varepsilon_t]
$$

Comme $\hat{\mathbb{V}}[\hat{X}_{t+h}]$ suit une loi normale centrée en tant que produit de $\varepsilon_t$ par un polynôme déterminé, la région de confiance de niveau $\alpha$ vérifie :

$$
IC_{0.95}(\mathbb{V}[\hat{X}_{t+h}]) = \left[ \hat{X}_{t+h} \pm 1.96\sqrt{\hat{\mathbb{V}}[\hat{X}_{t+h}]} \right]
$$

Ces simplifications permettent d'étendre l'horizon de prédiction au-delà de 2. Elles consistent à négliger la dépendance entre les prédictions de différents horizons, ce qui contribue à amplifier la variance estimée. Elles sont justifiées dans la mesure où $P$ n'est pas très différent de 1. On vérifie en particulier :

$$
ar_0=1 \Rightarrow P(1) = 1
$$

$$
\hat{ar}_{1} = -0,61193077 \Rightarrow P(2) = (ar_0+\hat{ar}_{1})^2 + 1 \approx 1,15 \Rightarrow \frac{1}{\sqrt{P(2)}} \approx 0,93
$$

## 3. Représentation graphique

On commence par préparer un dataframe pour stocker les valeurs prédites et les intervalles de confiance.

```{r}
data$log_values <- log(data$values)
data$var <- NA
data$CIlow <- NA
data$CIup <- NA
data$CIlow[nrow(data)] <- data$values[nrow(data)]
data$CIup[nrow(data)] <- data$values[nrow(data)]

arima1110$coef["ar0"] <- 1
```

On écrit une fonction qui élimine automatiquement les coefficients non-significatifs au seuil de 5 %, et qui applique une transformation exponentielle aux prévisions et aux bornes de l'intervalle de confiance afin de retrouver des valeurs cohérentes avec la série initiale.

```{r}
forecast <- function(h) {
  # On isole les coefficients statistiquement significatifs.
  sig_coefs <- c()
  for (i in 1:11) {
    coef <- arima1110$coef[paste0("ar", i)]
    se <- sqrt(arima1110$var.coef[paste0("ar", i), paste0("ar", i)])
    t <- coef/se
    pval <- (1 - pnorm(abs(t)))*2
    if (pval <= 0.05) {sig_coefs <- c(sig_coefs, i)}
    else {cat("ar", i, " rejected, p-value = ", pval, "\n", sep="")}
  }
  # On itère sur les horizons de prévision.
  for (j in 1:h) {
    T <- nrow(data)
    # On itère sur les coefficients significatifs pour calculer la valeur prédite.
    prev <- data$log_values[T]
    for (k in sig_coefs) {prev <- prev + as.numeric(arima1110$coef[paste0("ar", k)]) * (data$log_values[T-k+1] - data$log_values[T-k])}
    # On calcule la variance puis l'intervalle de confiance.
    if (j == 1) {var <- arima1110$sigma2}
    if (j > 1) {
      var <- 0
      for (l in 0:(j-2)) {var <- var + (((arima1110$coef[paste0("ar", l)] + arima1110$coef[paste0("ar", l+1)])**2) * data$var[T-l]) + arima1110$sigma2}
    }
    CIlow <- prev - 1.96*sqrt(var)
    CIup <- prev + 1.96*sqrt(var)
    data <<- rbind(data, data.frame(
      dates = (max(data$dates)) + (1/12), 
      values = exp(prev), 
      log_values = prev, 
      var = var, 
      CIlow = exp(CIlow), 
      CIup = exp(CIup)))
  }
  cat("\nForecast complete for h = ", h, ".", sep="")
}
```

On prédit jusqu'à l'horizon $h=6$ et l'on représente graphiquement les résultats (le paramètre $r$ permet de choisir la durée représentée, prévisions incluses, soit ici 24 mois d'observations et 6 mois de prévision).

```{r, warning=FALSE}
forecast(6)
rownames(data) <- NULL
print(tail(data[, -which(names(data) == "log_values")], 10))

r <- 30
subset_data <- data[(nrow(data) - r):nrow(data), ]

ggplot(data = subset_data) +
  geom_line(
    data = subset_data[0:(nrow(subset_data)-6), ],
    aes(x=dates, y=values, color = "obs")) +
  geom_line(
    data = subset_data[(nrow(subset_data)-6):nrow(subset_data), ],
    aes(x=dates, y=values, color = "prev")) +
  geom_line(aes(x=dates, y = CIlow), color = "limegreen", linetype = "dashed") +
  geom_line(aes(x=dates, y = CIup), color = "limegreen", linetype = "dashed") +
  geom_ribbon(aes(x=dates, ymin = CIlow, ymax = CIup, fill = "IC"), alpha = 0.5) +
  scale_color_manual(
    values = c("obs" = "skyblue", "prev" = "darkorange"),
    labels = c("obs" = "Observations", "prev" = "Prévisions")) +
  scale_fill_manual(
    values = c("IC" = "lightgray"),
    labels = c("IC" = "IC 95 %")) +
  labs(
    title = "Production de l'industrie pharmaceutique", 
    subtitle = "Prévisions à 6 mois", 
    x = "",
    y = "", 
    color = "",
    fill = "") +
  guides(color = guide_legend(order = 1), fill = guide_legend(order = 2)) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5), 
    plot.subtitle = element_text(hjust = 0.5), 
    legend.position = "bottom")
```

On procède à une vérification avec la fonction forecast du package éponyme : on retrouve les mêmes coefficients, mais des intervalles de confiance de plus en plus étroits, surtout pour $h>2$. Ceci confirme le caractère conservateur de nos hypothèses de calcul, dont l'effet est imperceptible pour $h\le2$.

```{r}
# On supprimer la fonction précédemment définie et on génère à nouveau le modèle.
rm(forecast)
arima1110 <- arima(log_series, c(11,1,0), include.mean=F)

# On génère la prédiction et l'on applique une transformation exponentielle pour obtenir des valeurs cohérentes avec la série initiale.
forecast <- forecast(arima1110, h = 6, level = 95)
forecast$mean <- exp(forecast$mean)
forecast$lower <- exp(forecast$lower)
forecast$upper <- exp(forecast$upper)

print(forecast)

# On compare la laergeur des IC obtenus.
forecasted_var <- cbind(
  data$values[!is.na(data$var)] - data$CIlow[!is.na(data$var)],
  as.data.frame(coredata(forecast$mean-forecast$lower)))
colnames(forecasted_var) <- c("own", "forecast package")
forecasted_var
```

## 4. Question ouverte

Imaginons qu'il existe une série $Y_t$ stationnaire entre $t=1$ et $T$ et telle que $Y_{T+1}$ soit disponible plus rapidement que $X_{T+1}$ : $Y_{T+1}$ permet d'améliorer la prévision de $X_{T+1}$ si $(Y_t)$ est corrélée instantanément avec $(X_t)$. Cette relation s'écrit :

$$
\hat{Y}_{T+1|\{X_u,Y_u,u≤t\}∪\{X_{T+1}\}}≠\hat{Y}_{T+1|\{Y_u,u≤t\}}
$$

On peut vérifier cette hypothèse à l'aide d'un test de Wald.
