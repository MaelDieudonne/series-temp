**Pré-requis**

```{r include=FALSE}
packages <- c("aTSA", "forecast", "ggplot2", "xts")
installed_packages <- packages %in% rownames(installed.packages())

if (any(installed_packages == FALSE)) {install.packages(packages[!installed_packages])}
invisible(lapply(packages, library, character.only = TRUE))

rm(installed_packages, packages)
```

**Téléchargement puis extraction des données**

La série étudiée est l'indice CVS-CJO de la production industrielle dans l'industrie pharmaceutique (base 100 en 2021).

```{r include=FALSE}
file_url <- "https://www.insee.fr/fr/statistiques/serie/telecharger/csv/010767832?ordre=antechronologique&transposition=donneescolonne&periodeDebut=1&anneeDebut=1990&periodeFin=2&anneeFin=2024&revision=sansrevisions"
name <- "Production de l'industrie pharmaceutique"

local_file_name <- "data.zip"
download.file(file_url, local_file_name, mode = "wb", quiet = TRUE)

file_list <- unzip(local_file_name)
file.rename(file_list[2], "valeurs_mensuelles.csv")

file.remove("data.zip")
file_list[2] <- sub("^\\.\\/", "", file_list[2])
file_list[2] <- sub("/valeurs_mensuelles\\.csv$", "", file_list[2]) 
unlink(file_list[2], recursive = TRUE)

rm(file_url, local_file_name, file_list)
```

**Préparation des données**

On supprime l'entête, on s'assure du nom et du format des colonnes, on tronque la série en décembre 2019 (pour éliminer les fluctuation intempestives dues au covid) et on la convertit au format ts.

```{r}
data <- read.csv("valeurs_mensuelles.csv", sep = ";")
data <- data[-c(1:3), -3]
colnames(data) <- c("dates", "values")

data <- data[order(data$dates), ]
rownames(data) <- NULL

data$dates <- as.yearmon(data$dates)
data$values <- as.numeric(data$values)

data <- data[1:which(data$dates == "Dec 2019"), ]

raw_series <- ts(data$values, start = min(data$dates), frequency = 12)
```

# Partie I : données

## 1. Représentation de la série

```{r}
plot(raw_series, main = name, xlab = "Dates", ylab = "Valeurs")
plot(decompose(raw_series), xlab = "")
```

On remarque une tendance linéaire croissante, une augmentation progressive de la variance, et l'absence de saisonnalité.

## 2. Stationnarisation

On commence par une transformation logarithmique pour corriger l'hétéroscédasticité.

```{r}
log_series <- log(raw_series)
plot(decompose(log_series), xlab = "")
```

On différencie ensuite à l'ordre 1 pour éliminer la tendance.

```{r}
diff_series <- diff(log_series, 1)
plot(decompose(diff_series), xlab = "")
```

On vérifie que le résultat convient à l'aide des tests usuels de stationnarité. La tendance a disparu mais il reste une légère dérive. On retient donc les tests avec dérive et sans tendance.

```{r eval=FALSE}
cat(paste("Dérive de la série :", mean(diff_series), "\n"))

adf <- adf.test(coredata(diff_series), nlag = 24, output = FALSE)
pp <- pp.test(coredata(diff_series), lag.short = TRUE, output = FALSE)
kpss <- kpss.test(coredata(diff_series), lag.short = TRUE, output = FALSE)

cat("\nTest ADF with drift no trend\n")
print(adf$type2)
cat("\nTest PP with drift no trend\n")
print(pp["type 2", ])
cat("\nTest KPSS with drift no trend\n")
print(kpss["type 2", ])
```

Les résultats permettent de rejeter l'hypothèse nulle de non-stationnarité pour les test ADF et PP, mais pas l'hypothèse nulle de stationnarité avec le test KPSS.

## 3. Comparaison de la série initiale et de la série transformée

```{r}
# par(mfrow = c(2, 1)) 
plot(raw_series, main = "Série initiale", xlab = "", ylab = "")
plot(diff_series, main = "Série transformée", xlab = "", ylab = "")
```

# Partie II : Modèle ARMA

## 1. Identification des paramètres

On représente les fonctions d'autocorrélation et d'auto-corrélation partielle.

```{r}
# par(mfrow = c(2, 1))
acf(diff_series, main = paste(name))
pacf(diff_series, main = paste(name))
```

On retient les valeurs de 1 pour *q* et 11 pour *p*. Ce n'est pas l'approche la plus parcimonieuse, mais le pic apparaissant au 11e retard pour les auto-corrélations partielles est bien prononcé (s'il apparaissait au 12e, on se serait plutôt inquiété d'une saisonnalité non-corrigée).

## 2. Estimation des modèles

On compare les modèles possibles selon les critères d'information : c'est un AR(11) qui minimise l'AIC et un MA(1) qui minimise le BIC (ce qui est logique puisque le BIC pénalise davantage la complexité).

```{r}
pmax <- 11
qmax <- 1
mat <- matrix(NA, nrow=pmax+1,ncol=qmax+1) # matrice vide à remplir
rownames(mat) <- paste0("p=",0:pmax) # renomme les lignes
colnames(mat) <- paste0("q=",0:qmax) # renomme les colonnes
AICs <- mat # matrice des AIC non remplie
BICs <- mat # matrice des BIC non remplie

pqs <- expand.grid(0:pmax,0:qmax) # toutes les combinaisons possibles de p et q
for (row in 1:dim(pqs)[1]){ # boucle pour chaque (p,q)
  p <- pqs[row,1] # récupère p
  q <- pqs[row,2] # récupère q
  estim <- try(arima(log_series,c(p,1,q),include.mean = F)) # tente d’estimer l’ARIMA
  AICs[p+1,q+1] <- if (class(estim)=="try-error") NA else estim$aic # assigne l’AIC
  BICs[p+1,q+1] <- if (class(estim)=="try-error") NA else BIC(estim) # assigne le BIC
}

print(AICs)
lowest_cell <- which(as.matrix(AICs) == min(as.matrix(AICs)), arr.ind = TRUE)
p <- rownames(AICs)[lowest_cell[1]]
p <- sub("^p=", "", p)
q <- colnames(AICs)[lowest_cell[2]]
q <- sub("^q=", "", q)
cat(paste0("\nBest model according to AIC is an ARMA(",p, ",", q, ").\n\n"))

print(BICs)
lowest_cell <- which(as.matrix(BICs) == min(as.matrix(BICs)), arr.ind = TRUE)
p <- rownames(BICs)[lowest_cell[1]]
p <- sub("^p=", "", p)
q <- colnames(BICs)[lowest_cell[2]]
q <- sub("^q=", "", q)
cat(paste0("\nBest model according to BIC is an ARMA(",p, ",", q, ")."))
```

On compare les modèles possibles selon l'erreur de prédiction calculées sur 5 dernières valeurs : c'est un ARMA(0,0) qui la minimise, c'est-à-dire un bruit blanc.

```{r}
values <- data$values[(nrow(data) - 4):nrow(data)]
pqs <- expand.grid(0:pmax,0:qmax)
mat <- matrix(NA, nrow=pmax+1, ncol=qmax+1)
rownames(mat) <- paste0("p=",0:pmax)
colnames(mat) <- paste0("q=",0:qmax)
pred_error <- mat 
for (row in 1:dim(pqs)[1]){
  p <- pqs[row,1]
  q <- pqs[row,2]
  pred_error[p+1,q+1] <- sqrt(mean((predict(arima(diff_series,c(p,1,q),include.mean=F),n.ahead = 5)$pred[1:5] - values)^2))
}

pred_error
lowest_cell <- which(as.matrix(pred_error) == min(as.matrix(pred_error)), arr.ind = TRUE)
p <- rownames(BICs)[lowest_cell[1]]
p <- sub("^p=", "", p)
q <- colnames(BICs)[lowest_cell[2]]
q <- sub("^q=", "", q)
cat(paste0("\nBest model according to prediction error is an ARMA(",p, ",", q, ")."))
```

On retient ces trois modèles afin de vérifier leur validité et leur ajustement.

```{r}
arima1110 <- arima(log_series,c(11,1,0),include.mean=F)
arima011 <- arima(log_series,c(0,1,1),include.mean=F)
arima010 <- arima(log_series,c(0,1,0),include.mean=F)
```

## 3. Tests de validité et d'ajustement

Concernant d'abord l'ajustement, on vérifie la significativité des coefficients les plus élevés : les modèles AR(11) et MA(1) passent ce test, qui n'a évidemment pas de sens pour un bruit blanc.

```{r}
signif <- function(estim){
  coef <- estim$coef
  se <- sqrt(diag(estim$var.coef))
  t <- coef/se
  pval <- (1 - pnorm(abs(t)))*2
  return(rbind(coef, se, pval))
}

signif(arima1110)
cat("\n")
signif(arima011)
```

On considère maintenant la validité, c'est-à-dire l'absence d'autocorrélation entre les résidus. On utilise un test Portemanteau, conduisant à rejeter le MA(1) et l'ARMA(0,0).

```{r}
Qtests <- function(series, k, fitdf = 0) {
  pvals <- apply(matrix(1:k), 1, FUN = function(l) {
    pval <- if (l <= fitdf) NA else Box.test(series, lag = l, type = "Ljung-Box", fitdf = fitdf)$p.value
    return(c("lag" = l,"pval" = pval))
  })
  return(t(pvals))
}

cat("ARMA(11,0)\n")
Qtests(arima1110$residuals, 24, fitdf=11)
cat("\nARMA(0,1)\n")
Qtests(arima011$residuals, 24, fitdf=1)
cat("\nARMA(0,0)\n")
Qtests(arima010$residuals, 24, fitdf=0)
```

On procède à une dernière vérification avec la fonction auto.arima() du package forecast : elle retient le MA(1) que l'on vient d'écarter.

```{r}
auto.arima(log_series, trace = TRUE)
```

# Partie III : Prévision

On commence par préparer un dataframe pour stocker les valeurs prédites et les intervalles de confiance.

```{r}
data$log_values <- log(data$values)
data$CIlow <- NA
data$CIup <- NA
data$CIlow[nrow(data)] <- data$values[nrow(data)]
data$CIup[nrow(data)] <- data$values[nrow(data)]
```

On passe à la prédiction à l'aide du modèle AR(11) retenu. Il s'écrit ainsi :

$$
\nabla X_{t+1} = ar_1 \nabla X_t + ar_2 \nabla X_{t-1} + \cdots + ar_{11} \nabla X_{t-10} + \varepsilon_t
$$

On en déduit l'expression des termes prédits :

$$
\hat{X}_{t+1} = E(X_{t+1}|X_t,\ldots,X_{t-11}) = X_t + \hat{ar}_1(X_t - X_{t-1}) + \hat{ar}_2(X_{t-1} - X_{t-2}) + \cdots + \hat{ar}_{11}(X_{t-10} - X_{t-11}) \\
\hat{X}_{t+2} = E(X_{t+2}|\hat{X}_{t+1},X_t,\ldots,X_{t-10}) = \hat{X}_{t+1} + \hat{ar}_1(\hat{X}_{t+1} - X_{t}) + \hat{ar}_2(X_{t} - X_{t-1}) + \cdots + \hat{ar}_{11}(X_{t-9} - X_{t-10})
$$

On élimine les coefficients ar5, ar6 et ar7 qui ne sont pas significatifs au seuil de 10 %, comme on l'a vu plus haut. Pour calculer l'intervalle de confiance, on néglige l'incertitude entourant les coefficients estimés, pour considérer seulement la variance des résidus. On écrit une fonction qui permet d'étendre facilement l'horizon de prédiction. On retient 6 mois, au prix d'un intervalle de confiance croissant fortement.

**Je ne suis pas complètement sûr concernant l'IC, il faudrait peut-être plutôt :**

-   **h=1: sigma2**

-   **h=2: (1+ar1)\*sigma2**

-   **h=3: (1+ar2)(1+ar1)\*sigma2**

-   **etc.**

```{r}
forecast <- function(h) {
  for (j in 1:h) {
    T <- nrow(data)
    prev <- data$log_values[T]
    for (i in c(1:4, 8:11)) {
      prev <- prev + as.numeric(arima1110$coef[paste0("ar", i)]) * (data$log_values[T-i+1] - data$log_values[T-i])}
    CIlow <- prev - j*1.96*sqrt(arima1110$sigma2)
    CIup <- prev + j*1.96*sqrt(arima1110$sigma2) 
    data <<- rbind(
      data,
      data.frame(dates = (max(data$dates)) + (1/12), values = exp(prev), log_values = prev, CIlow = exp(CIlow), CIup = exp(CIup)))
  }
}

forecast(6)
```

On représente finalement les résultats. Le paramètre *r* permet de choisir la durée représentée, prédictions incluses, soit ici 24 mois d'observations et 6 mois de prédiction.

```{r}
r <- 30
subset_data <- data[(nrow(data) - r):nrow(data), ]

ggplot(data = subset_data) +
  geom_line(data = subset_data[0:(nrow(subset_data)-6), ], aes(x=dates, y=values, color = "obs")) +
  geom_line(data = subset_data[(nrow(subset_data)-6):nrow(subset_data), ], aes(x=dates, y=values, color = "pred")) +
  geom_line(aes(x=dates, y = CIlow), color = "darkgrey", linetype = "dashed") +
  geom_line(aes(x=dates, y = CIup), color = "darkgrey", linetype = "dashed") +
  geom_ribbon(aes(x=dates, ymin = CIlow, ymax = CIup, fill = "IC"), alpha = 0.5) +
  scale_color_manual(values = c("obs" = "skyblue", "pred" = "darkorange"), labels = c("obs" = "Observations", "pred" = "Prédictions")) +
  scale_fill_manual(values = c("IC" = "lightgray"), labels = c("IC" = "95 % IC")) +
  labs(
    title = "Production de l'industrie pharmaceutique", 
    subtitle = "Prévisions à m+6", 
    x = "",
    y = "", 
    color = "",
    fill = "") +
  guides(color = guide_legend(order = 1), fill = guide_legend(order = 2)) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5), 
    plot.subtitle = element_text(hjust = 0.5), 
    legend.position = "bottom")
```
