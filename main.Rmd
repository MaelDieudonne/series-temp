**Pré-requis**

```{r include=FALSE}
packages <- c("astsa", "aTSA", "forecast", "fUnitRoots", "ggplot2", "MASS", "xts")
installed_packages <- packages %in% rownames(installed.packages())

if (any(installed_packages == FALSE)) {install.packages(packages[!installed_packages])}
invisible(lapply(packages, library, character.only = TRUE))

rm(installed_packages, packages)
```

**Téléchargement puis extraction des données**

Série traitée : Indice CVS-CJO de la production industrielle dans l'industrie pharmaceutique (base 100 en 2021)

```{r include=FALSE}
file_url <- "https://www.insee.fr/fr/statistiques/serie/telecharger/csv/010767832?ordre=antechronologique&transposition=donneescolonne&periodeDebut=1&anneeDebut=1990&periodeFin=2&anneeFin=2024&revision=sansrevisions"
name <- "Production de l'industrie pharmaceutique"

local_file_name <- "data.zip"
download.file(file_url, local_file_name, mode = "wb", quiet = TRUE)

file_list <- unzip(local_file_name)
file.rename(file_list[2], "valeurs_mensuelles.csv")

file.remove("data.zip")
file_list[2] <- sub("^\\.\\/", "", file_list[2])
file_list[2] <- sub("/valeurs_mensuelles\\.csv$", "", file_list[2]) 
unlink(file_list[2], recursive = TRUE)

rm(file_url, local_file_name, file_list)
```

**Conversion au format ts**

```{r}
data <- read.csv("valeurs_mensuelles.csv", sep = ";")
data <- data[-c(1:3), -3]
colnames(data) <- c("dates", "values")

data <- data[order(data$dates), ]
rownames(data) <- NULL

data$dates <- as.yearmon(data$dates)
data$values <- as.numeric(data$values)

# On tronque la série après 2019 pour éviter les flucutations intempestives dues au covid, surtout compte-tenu de l'objet de notre série.
data <- data[1:which(data$dates == "Dec 2019"), ]

raw_series <- ts(data$values, start = min(data$dates), frequency = 12)
```

# Partie I : données

## 1. Représentation de la série

```{r}
plot(raw_series, main = name, xlab = "Dates", ylab = "Valeurs")
plot(decompose(raw_series))
```

Trois observations : une tendance linéaire croissante, une augmentation de la variance au cours du temps, aucune de saisonnalité. Pas besoin de régression linéaire pour objectiver la tendance, elle est vraiment flagrante.

## 2. Stationnarisation

On commence par une transformation logarithmique pour corriger l'hétéroscédasticité.

```{r}
log_series <- log(raw_series)
plot(decompose(log_series))
```

On différencie ensuite à l'ordre 1 pour éliminer la tendance.

```{r}
diff_series <- diff(log_series, 1)
plot(decompose(diff_series))
```

On vérifie que le résultat convient à l'aide de tests de stationnarité. La tendance a disparu mais il reste une légère dérive. On retient donc les tests avec drift sans trend. La série passe les trois.

```{r eval=FALSE}
cat(paste("Moyenne de la série :", mean(diff_series), "\n"))

adf <- adf.test(coredata(diff_series), nlag = 24, output = FALSE)
pp <- pp.test(coredata(diff_series), lag.short = TRUE, output = FALSE)
kpss <- kpss.test(coredata(diff_series), lag.short = TRUE, output = FALSE)

cat("\nTest ADF with drift no trend\n")
print(adf$type2)
cat("\nTest PP with drift no trend\n")
print(pp["type 2", ])
cat("\nTest KPSS with drift no trend\n")
print(kpss["type 2", ])
```

## 3. Comparaison de la série initiale et de la série transformée

```{r}
# par(mfrow = c(2, 1)) 
plot(raw_series, main = "Série initiale", xlab = "", ylab = "")
plot(diff_series, main = "Série transformée", xlab = "", ylab = "")
```

# Partie II : Modèle ARMA

## 1. Identification des paramètres

```{r}
# par(mfrow = c(2, 1))
acf(diff_series, main = paste(name))
pacf(diff_series, main = paste(name))
```

On retient qmax=1 et pmax = 11. Ce n'est pas l'approche la plus parcimonieuse, mais le pic à l=11 est quand même bien prononcé (et pas à 12, auquel cas on se serait plutôt inquiété d'un problème de saisonnalité).

## 2. Estimation des modèles

Par les critères d'information : le modèle minimisant l'AIC est un ARMA(11,0) mais celui minimisant le BIC est un ARMA(0,1) (logique puisque le BIC pénalise davantage la complexité).

```{r}
pmax <- 11
qmax <- 1
mat <- matrix(NA, nrow=pmax+1,ncol=qmax+1) # matrice vide à remplir
rownames(mat) <- paste0("p=",0:pmax) # renomme les lignes
colnames(mat) <- paste0("q=",0:qmax) # renomme les colonnes
AICs <- mat # matrice des AIC non remplie
BICs <- mat # matrice des BIC non remplie

pqs <- expand.grid(0:pmax,0:qmax) # toutes les combinaisons possibles de p et q
for (row in 1:dim(pqs)[1]){ # boucle pour chaque (p,q)
  p <- pqs[row,1] # récupère p
  q <- pqs[row,2] # récupère q
  estim <- try(arima(log_series,c(p,1,q),include.mean = F)) # tente d’estimer l’ARIMA
  AICs[p+1,q+1] <- if (class(estim)=="try-error") NA else estim$aic # assigne l’AIC
  BICs[p+1,q+1] <- if (class(estim)=="try-error") NA else BIC(estim) # assigne le BIC
}

print(AICs)
lowest_cell <- which(as.matrix(AICs) == min(as.matrix(AICs)), arr.ind = TRUE)
p <- rownames(AICs)[lowest_cell[1]]
p <- sub("^p=", "", p)
q <- colnames(AICs)[lowest_cell[2]]
q <- sub("^q=", "", q)
cat(paste0("\nBest model according to AIC is an ARMA(",p, ",", q, ").\n\n"))

print(BICs)
lowest_cell <- which(as.matrix(BICs) == min(as.matrix(BICs)), arr.ind = TRUE)
p <- rownames(BICs)[lowest_cell[1]]
p <- sub("^p=", "", p)
q <- colnames(BICs)[lowest_cell[2]]
q <- sub("^q=", "", q)
cat(paste0("\nBest model according to BIC is an ARMA(",p, ",", q, ")."))
```

Par l'erreur de prédiction : le modèle qui la minimise, sur les 5 dernières valeurs, est un ARMA(0,0), donc un bruit blanc.

```{r}
values <- data$values[(nrow(data) - 4):nrow(data)]
pqs <- expand.grid(0:pmax,0:qmax)
mat <- matrix(NA, nrow=pmax+1, ncol=qmax+1)
rownames(mat) <- paste0("p=",0:pmax)
colnames(mat) <- paste0("q=",0:qmax)
pred_error <- mat 
for (row in 1:dim(pqs)[1]){
  p <- pqs[row,1]
  q <- pqs[row,2]
  pred_error[p+1,q+1] <- sqrt(mean((predict(arima(diff_series,c(p,1,q),include.mean=F),n.ahead = 5)$pred[1:5] - values)^2))
}

pred_error
lowest_cell <- which(as.matrix(pred_error) == min(as.matrix(pred_error)), arr.ind = TRUE)
p <- rownames(BICs)[lowest_cell[1]]
p <- sub("^p=", "", p)
q <- colnames(BICs)[lowest_cell[2]]
q <- sub("^q=", "", q)
cat(paste0("\nBest model according to prediction error is an ARMA(",p, ",", q, ")."))
```

```{r}
arima1110 <- arima(log_series,c(11,1,0),include.mean=F)
arima011 <- arima(log_series,c(0,1,1),include.mean=F)
arima010 <- arima(log_series,c(0,1,0),include.mean=F)
```

## 3. Tests de validité et d'ajustement

Pour l'ajustement, on vérifie la significativité des coefficients les plus élevés : l'ARMA(11,0) et l'ARMA(0,1) passent ce test (qui n'a évidemment pas de sens pour un bruit blanc).

```{r}
signif <- function(estim){
  coef <- estim$coef
  se <- sqrt(diag(estim$var.coef))
  t <- coef/se
  pval <- (1-pnorm(abs(t)))*2
  return(rbind(coef, se, pval))
}

signif(arima1110)
cat("\n")
signif(arima011)
```

Pour la validité, c'est-à-dire la non-autocorrélation des résidus, on utilise le test Portemanteau. Il conduit à rejeter l'ARMA(0,1) et l'ARMA(0,0).

```{r}
Qtests <- function(series, k, fitdf = 0) {
  pvals <- apply(matrix(1:k), 1, FUN = function(l) {
    pval <- if (l <= fitdf) NA else Box.test(series, lag = l, type = "Ljung-Box", fitdf = fitdf)$p.value
    return(c("lag" = l,"pval" = pval))
  })
  return(t(pvals))
}

cat("ARMA(11,0)\n")
Qtests(arima1110$residuals, 24, fitdf=11)
cat("\nARMA(0,1)\n")
Qtests(arima011$residuals, 24, fitdf=1)
cat("\nARMA(0,0)\n")
Qtests(arima010$residuals, 24, fitdf=0)
```

Vérification avec la fonction auto.arima() : elle retient l'ARMA(0,1) qu'on vient d'écarter.

```{r}
auto.arima(log_series, trace = TRUE)
```

# Partie III : Prévision

Nous allons bien nous amuser avec un ARMA(11,0) ! On élimine les coefficients ar5, ar6 et ar7 qui n'étaient pas significatifs plus haut.

$$
\nabla\hat{X}_{t+1} = \hat{ar}_1 \nabla X_t + \hat{ar}_2 \nabla X_{t-1} + ... + \hat{ar}_{11} \nabla X_{t-10} \\
\hat{X}_{t+1} = X_t + \hat{ar}_1(X_t - X_{t-1}) + \hat{ar}_2(X_{t-1} - X_{t-2}) + ... + \hat{ar}_{11}(X_{t-10} - X_{t-11})
$$

```{r}
# Chunk pour réinitialiser, à supprimer
data <- read.csv("valeurs_mensuelles.csv", sep = ";")
data <- data[-c(1:3), -3]
colnames(data) <- c("dates", "values")
data <- data[order(data$dates), ]
rownames(data) <- NULL
data$dates <- as.yearmon(data$dates)
data$values <- as.numeric(data$values)
data <- data[1:which(data$dates == "Dec 2019"), ]
raw_series <- ts(data$values, start = min(data$dates), frequency = 12)
log_series <- log(raw_series)
arima1110 <- arima(log_series,c(11,1,0),include.mean=F)
```

```{r}
data$log_values <- log(data$values)
data$CIlow <- NA
data$CIup <- NA
```

```{r}
forecast <- function(h) {
  for (j in 1:h) {
    T <- nrow(data)
    prev <- data$log_values[T]
    for (i in c(1:4, 8:11)) {
      prev <- prev + as.numeric(arima1110$coef[paste0("ar", i)]) * (data$log_values[T-i+1] - data$log_values[T-i])}
    CIlow <- prev - j*1.96*sqrt(arima1110$sigma2)
    CIup <- prev + j*1.96*sqrt(arima1110$sigma2) 
    data <<- rbind(
      data,
      data.frame(dates = (max(data$dates)) + (1/12), values = exp(prev), log_values = prev, CIlow = exp(CIlow), CIup = exp(CIup)))
  }
}
forecast(5)

subset_data[(nrow(data) - 25):nrow(data), ]

ggplot(data = subset_data, aes(x=dates, y=values)) +
  geom_line() +
  geom_ribbon(aes(ymin = CIlow, ymax = CIup), fill = "lightgray", alpha = 0.5) +
  geom_line(aes(y = CIlow), color = "lightgreen", linetype = "dashed") +
  geom_line(aes(y = CIup), color = "lightgreen", linetype = "dashed") +
  labs(title = "Prévisions à t+5", x = "", y = "") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

ggplot(data = subset_data, aes(x = dates, y = values)) +
  geom_line(aes(colour = ifelse(seq_along(values) > (length(values) - 5), "Prédictions", "Observations")), size = 0.5) +
  geom_ribbon(aes(ymin = CIlow, ymax = CIup), fill = "lightgray", alpha = 0.5) +
  geom_line(aes(y = CIlow), color = "lightgreen", linetype = "dashed") +
  geom_line(aes(y = CIup), color = "lightgreen", linetype = "dashed") +
  scale_colour_manual(values = c("blue", "black"), guide = FALSE) +
  labs(title = "Prévisions à t+5", x = "", y = "") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```
